---
title: "STATS506 - Homework 4"
author: "Prathibha Muthukumara Prasanna"
format: 
  html:
    self-contained: true
    toc: true
    theme: cosmo
    number-sections: true
execute:
  echo: true
  warning: false
  message: false

---

## Problem 1 - Tidyverse: New Zealand

#### Problem 1a
```{r}
library(tidyverse)
library(nzelect)

#Load the New Zealand Election data
data(nzge)
head(nzge)

vote_counts <- nzge |>
  #Group by year and voting type
  group_by(election_year, voting_type) |>  
  #Sum all votes within each group, removing any NA values
  summarise(total_votes = sum(votes, na.rm = TRUE), .groups = "drop") |> 
  # Sort by total votes in ascending order
  arrange(total_votes)
print(vote_counts)

```
This table shows the total number of votes cast in each election year, separated by vote type (“Candidate” vs. “Party”). Data  was grouped by year and vote type, and all votes in each group was summed. The results were sorted.

#### Problem 1b
```{r}
candidate_2014 <- nzge |>
  #Filter for 2014 and Candidate votes only
  filter(election_year == 2014, voting_type == "Candidate") |>
  #Group by party
  group_by(party) |>
  #Sum votes for each party
  summarise(votes = sum(votes, na.rm = TRUE), .groups = "drop") |>
  #Calculate proportion and percentage
  mutate(proportion = votes / sum(votes),
         percentage = proportion * 100
         ) |>
  #Sort by percent in ascending order
  arrange(percentage)
print(candidate_2014)
```
The filtered the dataset includes only Candidate votes from the 2014 election. Data was then grouped by political party and each party’s total Candidate votes were calculated. To understand how much support each party received, the following were calculated:
Proportion = votes for that party ÷ total Candidate votes in 2014
Percentage = proportion × 100
The table is sorted by percentage.

#### Problem 1c 
```{r}
winners <- nzge |>
  group_by(election_year, voting_type, party) |>
  summarise(totalvotes = sum(votes, na.rm = TRUE), .groups = "drop") |>
  group_by(election_year, voting_type) |>
  slice_max(totalvotes, n = 1) |>
  ungroup()

#Pivot to wide format 
winners_wide <- winners |>
  select(election_year, voting_type, party) |>
  pivot_wider(
    names_from = voting_type,
    values_from = party
  ) |>
  arrange(election_year)

print(winners_wide)

```
Votes were summed for each party within each election year and vote type. The party with the highest number of votes was selected using slice_max(). The output data was reshaped into a wide table.


## Problem 2 - Tidyverse: Tennis

#### Problem 2a
```{r}
#Load ATP tennis match data for 2019
url <- "https://raw.githubusercontent.com/JeffSackmann/tennis_atp/refs/heads/master/atp_matches_2019.csv"
tennis <- read_csv(url)
head(tennis)
#str(tennis)

tournament_count <- tennis |>
  #Extract year from YYYYMMDD
  mutate(year = substr(tourney_date, 1, 4)) |>
  #Keep only tournaments from 2019
  filter(year == "2019") |>                      
  distinct(tourney_id) |>          
  count(name = "num_tournaments")

tournament_count
```
str(tennis) shows that the dataset contains 2,806 matches and 49 variables. Each row represents a single match. 
The year from tourney_date was extracted, filtered for 2019, and the number of unique tournament IDs were counted. The output shows that 125 ATP tournaments took place in 2019.

#### Problem 2b 
```{r}
tournament_wins <- tennis |>
  mutate(year = substr(tourney_date, 1, 4)) |>
  #Only keep finals
  filter(year == "2019", round == "F") |>
  # Count tournaments won by each player
  group_by(winner_name) |>
  summarise(tournaments_won = n(), .groups = 'drop') |>
  arrange(desc(tournaments_won))

tournament_wins
```
The dataset was filtered to include only final-round matches (round == "F"). This ensures only tournament champions are counted rather than match winners. Player names were grouped and number of finals won by each player was counted.

```{r}
players_multiple <- tournament_wins |>
  filter(tournaments_won > 1)

players_multiple
```
12 players won more than one tournament in 2019.

```{r}
most_titles <- tournament_wins |>
  slice_max(tournaments_won, n = 1)

most_titles
```
The maximum number of tournaments won by any player was 5.
Two players achieved this record number of wins: Dominic Thiem and Novak Djokovic

#### Problem 2c 
```{r}
library(tidyverse)
library(infer)

#Reshape into long format
aces_long <- tennis |>
  mutate(year = substr(tourney_date, 1, 4)) |>
  filter(year == "2019") |>
  select(w_ace, l_ace) |>
  pivot_longer(cols = c(w_ace, l_ace),
               names_to = "result",
               values_to = "aces") |>
  mutate(result = if_else(result == "w_ace", "Winner", "Loser"))

#Compute group means
aces_summary <- aces_long |>
  group_by(result) |>
  summarise(mean_aces = mean(aces, na.rm = TRUE))

aces_summary

#Calculate observed difference
obs_diff <- aces_long |>
  specify(aces ~ result) |>
  calculate(stat = "diff in means", order = c("Winner", "Loser"))

#Generate null distribution via permutation
null_distribution <- aces_long |>
  specify(aces ~ result) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "diff in means", order = c("Winner", "Loser"))

#Calculate p-value
p_value <- null_distribution |>
  get_p_value(obs_stat = obs_diff, direction = "right")

obs_diff
p_value

```
There is very strong statistical evidence that winners have more aces than losers.

Hypothesis Test:
Method: Permutation test with 1,000 replications using the infer package
Null Hypothesis (H₀): Winners and losers have the same mean number of aces
Alternative Hypothesis (Hₐ): Winners have more aces than losers (one-sided test)
Test statistic: Difference in means (Winner - Loser) = 1.683525
p-value: < 0.001 (reported as 0, indicating none of the 1,000 permutations produced a difference as large as the observed difference)

With a p-value < 0.001, we have extremely strong evidence to reject the null hypothesis. Winners hit significantly more aces than losers (approximately 1.68 more aces per match on average). 


#### Problem 2d
```{r}

#Create a dataset of wins and losses in long format
winrate_data <- tennis |>
  mutate(year = substr(tourney_date, 1, 4)) |>
  filter(year == "2019") |>
  select(winner_name, loser_name) |>
  pivot_longer(
    cols = c(winner_name, loser_name),
    names_to = "match_outcome",
    values_to = "player"
  ) |>
  mutate(win_flag = if_else(match_outcome == "winner_name", 1, 0))

#Compute wins, total matches, and win-rate
winrate_summary <- winrate_data |>
  group_by(player) |>
  summarise(
    matches_played = n(),
    matches_won = sum(win_flag),
    win_rate = matches_won / matches_played
  ) |>
  filter(matches_played >= 5) |>          
  arrange(desc(win_rate))

winrate_summary

```
Win-rate was calculated by dividing the number of matches won by the total number of matches played (wins + losses). The dataset was reshaped so that each record represented one match for one player, with a binary indicator showing whether that player won. Only players who participated in at least five matches were included in the analysis.

```{r}
highest_winrate <- winrate_summary |>
  slice_max(win_rate)

highest_winrate
```
The highest win-rate in 2019 belonged to Rafael Nadal, with 60 wins out of 69 matches, resulting in a win-rate of about 0.87 among players who played at least five matches.


## Problem 3 - Visualization

```{r}
library(lubridate)
library(scales)

covid_data <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/rolling-averages/us-states.csv")

#Convert date to Date format
covid_data <- covid_data %>%
  mutate(date = ymd(date))

#Aggregate national data
national_data <- covid_data %>%
  group_by(date) %>%
  summarise(
    total_cases = sum(cases, na.rm = TRUE),
    total_cases_avg = sum(cases_avg, na.rm = TRUE),
    total_cases_avg_per_100k = sum(cases_avg_per_100k, na.rm = TRUE)
  )


p1 <- ggplot(national_data, aes(x = date, y = total_cases_avg)) +
  geom_line(color = "#2E5090", size = 1) +
  geom_area(fill = "#2E5090", alpha = 0.3) +
  scale_y_continuous(labels = comma, expand = c(0, 0)) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b\n%Y") +
  labs(
    title = "Major Waves of COVID-19 in the United States",
    subtitle = "Five distinct waves visible in national 7-day rolling average of new cases",
    x = NULL,
    y = "Daily New Cases (7-day average)",
    caption = "Data: The New York Times COVID-19 Database"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(color = "gray40", size = 11, margin = margin(b = 10)),
    plot.caption = element_text(color = "gray50", hjust = 0, size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.text = element_text(color = "gray30"),
    plot.margin = margin(10, 20, 10, 10)
  )

print(p1)
ggsave("plot1_national_waves.png", p1, width = 10, height = 6, dpi = 300)
```
The national COVID-19 time series displays five major spikes (Apr 2020, Aug 2020, Jan 2021, Sep 2021, and Jan 2022) and two minor spikes (Jul 2022 and Jan 2023). The January 2022 wave is the tallest and most pronounced spike, exceeding all others.

``` {r}
#Calculate total cases per 100k for each state
state_totals <- covid_data %>%
  group_by(state) %>%
  summarise(
    total_cases_per_100k = sum(cases_avg_per_100k, na.rm = TRUE),
    max_date = max(date)
  ) %>%
  arrange(desc(total_cases_per_100k))

#Identify highest and lowest states
highest_state <- state_totals$state[1]
lowest_state <- state_totals$state[nrow(state_totals)]

#Filter data for these states
comparison_states <- covid_data %>%
  filter(state %in% c(highest_state, lowest_state)) %>%
  select(date, state, cases_avg_per_100k)


p2 <- ggplot(comparison_states, aes(x = date, y = cases_avg_per_100k, 
                                     color = state, fill = state)) +
  geom_line(size = 1.2) +
  geom_area(alpha = 0.2, position = "identity") +
  scale_color_manual(values = c("#D55E00", "#0072B2")) +
  scale_fill_manual(values = c("#D55E00", "#0072B2")) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b\n%Y") +
  scale_y_continuous(labels = comma) +
  labs(
    title = paste("COVID-19 Trajectories:", highest_state, "vs", lowest_state),
    subtitle = "States with highest and lowest cumulative case rates per 100,000 population",
    x = NULL,
    y = "Daily Cases per 100k (7-day average)",
    color = "State",
    fill = "State",
    caption = "Data: The New York Times COVID-19 Database"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(color = "gray40", size = 11, margin = margin(b = 10)),
    plot.caption = element_text(color = "gray50", hjust = 0, size = 9),
    legend.position = "bottom",
    legend.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.text = element_text(color = "gray30"),
    plot.margin = margin(10, 20, 10, 10)
  )

print(p2)
ggsave("plot2_state_comparison.png", p2, width = 10, height = 6, dpi = 300)
```
Rhode Island shows consistently higher daily case rates per 100,000 population across most of the timeline. The difference becomes especially large during the surge centered around January 2022, where Rhode Island reaches a sharp spike exceeding 500 cases per 100k on the 7-day average. Maine remains lower throughout the pandemic, with peaks that are smaller in height and slower in rise. Peaks in Maine also occur later relative to Rhode Island, indicating a slower spread. After mid-2022, both states show a gradual decline and stabilization at low levels.

``` {r}

#Define "substantial" as reaching 10 cases per 100k on 7-day average
threshold <- 10

#Find first date each state reached threshold
first_substantial <- covid_data %>%
  filter(cases_avg_per_100k >= threshold) %>%
  group_by(state) %>%
  summarise(first_date = min(date)) %>%
  arrange(first_date) %>%
  head(5)

early_states <- covid_data %>%
  filter(state %in% first_substantial$state) %>%
  filter(date <= as.Date("2020-06-01")) %>%  # Focus on early period
  mutate(state = factor(state, levels = first_substantial$state))

p3 <- ggplot(early_states, aes(x = date, y = cases_avg_per_100k, color = state)) +
  geom_line(size = 1.2) +
  geom_hline(yintercept = threshold, linetype = "dashed", 
             color = "gray40", size = 0.8) +
  scale_color_brewer(palette = "Set1") +
  scale_x_date(date_breaks = "1 month", date_labels = "%b\n%Y") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "First Five States to Experience Substantial COVID-19 Outbreak",
    subtitle = "Ordered by first date reaching 10 daily cases per 100k (7-day average)",
    x = NULL,
    y = "Daily Cases per 100k (7-day average)",
    color = "State (in order\nof outbreak)",
    caption = "Data: The New York Times COVID-19 Database\nDashed line indicates substantial outbreak threshold (10 cases per 100k)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(color = "gray40", size = 11, margin = margin(b = 10)),
    plot.caption = element_text(color = "gray50", hjust = 0, size = 9, lineheight = 1.2),
    legend.position = "right",
    legend.title = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.text = element_text(color = "gray30"),
    plot.margin = margin(10, 20, 10, 10)
  )

print(p3)
ggsave("plot3_first_five_states.png", p3, width = 10, height = 6, dpi = 300)

```
A threshold of 10 daily cases per 100,000 population (7-day average) marks the point at which community spread becomes substantial. Based on the plot, the earliest states to cross this threshold are: New York, 
New Jersey, Massachusetts, Connecticut, and Louisiana.These states cross the threshold between mid-March and early April 2020. The initial surge is sharp and steep, especially in New York and New Jersey. Louisiana appears shortly after. States outside this group reach the threshold later in the spring.





## Attribution of Sources 

Problem 1 - https://dplyr.tidyverse.org/reference/slice.html - slice_max()

Problem 2 - https://infer.netlify.app - library infer 

Problem 3 - https://scales.r-lib.org - library scales and https://lubridate.tidyverse.org - library lubridate 


## Github Repository 
https://github.com/prathii7/Computational-Methods-and-Tools-in-Statistics


